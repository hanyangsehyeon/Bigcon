import streamlit as st
import asyncio
import json

from mcp.client.stdio import stdio_client
from mcp import ClientSession, StdioServerParameters
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from PIL import Image
from pathlib import Path

# í™˜ê²½ë³€ìˆ˜
ASSETS = Path("assets")
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]

system_prompt = """
ë‹¹ì‹ ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆí•˜ëŠ” ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ë‹µë³€ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ JSON ì½”ë“œë§Œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.

JSON êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸(ë°°ì—´) í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤:
[
  {
    "section": "ë§ˆì¼€íŒ… ì „ëµì˜ ì£¼ì œë‚˜ ë‹¨ê³„",
    "content": "í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ êµ¬ì²´ì ì¸ ë§ˆì¼€íŒ… ì œì•ˆ ë‚´ìš©. ë§Œì•½ ë‚´ìš©ì— ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ê³¼ ê°™ì´ ì—¬ëŸ¬ ì¤„ì´ í¬í•¨ëœë‹¤ë©´, ë°˜ë“œì‹œ ì¤„ë°”ê¿ˆì„ '\\n' ìœ¼ë¡œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.",
    "basis": "í•´ë‹¹ ì œì•ˆì„ ë’·ë°›ì¹¨í•˜ëŠ” ë°ì´í„° ê¸°ë°˜ì˜ ê·¼ê±°"
  }
]

## ì˜ˆì‹œ ##
ë‹¤ìŒì€ contentì— ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ì´ í¬í•¨ëœ ê²½ìš°ì˜ ì˜¬ë°”ë¥¸ JSON í˜•ì‹ì…ë‹ˆë‹¤:
[
  {
    "section": "ì£¼ìš” ê³ ê° ë¶„ì„",
    "content": "ì•„ë˜ëŠ” ì£¼ìš” ê³ ê°ì¸µì˜ ì—°ë ¹ ë° ì„±ë³„ ë¶„í¬ì…ë‹ˆë‹¤.\\n\\n| ì—°ë ¹ëŒ€ | ë‚¨ì„± | ì—¬ì„± |\\n|---|---|---|\\n| 20ëŒ€ | 15% | 25% |\\n| 30ëŒ€ | 30% | 20% |\\n| 40ëŒ€ | 5% | 5% |",
    "basis": "ê°€ë§¹ì ì˜ ìµœê·¼ 3ê°œì›” ê²°ì œ ë°ì´í„°ë¥¼ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤."
  }
]

ê·œì¹™:
1. ëª¨ë“  ì¤„ë°”ê¿ˆì€ ë°˜ë“œì‹œ ë¬¸ìì—´ ë‚´ë¶€ì—ì„œ '\\n' ìœ¼ë¡œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬í•œë‹¤.
2. ì¶œë ¥ í…ìŠ¤íŠ¸ ëì—ëŠ” ì—­ìŠ¬ë˜ì‹œ(\\) ê°™ì€ ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì ˆëŒ€ ë„£ì§€ ì•ŠëŠ”ë‹¤.
3. JSON ì´ì™¸ì˜ ì„¤ëª…, ì½”ë“œë¸”ë¡, ì£¼ì„ì€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
4. ëª¨ë“  ì œì•ˆ(content)ì—ëŠ” ë°˜ë“œì‹œ ë°ì´í„°ì— ê¸°ë°˜í•œ ê·¼ê±°(basis)ê°€ í•¨ê»˜ ì œì‹œë˜ì–´ì•¼ í•œë‹¤.
"""
greeting = """ë§ˆì¼€íŒ…ì´ í•„ìš”í•œ ê°€ë§¹ì ì„ ì•Œë ¤ì£¼ì„¸ìš” \nì£¼ì†Œë„ í•¨ê»˜ ì…ë ¥í•´ì£¼ì‹œë©´, ê°€ë§¹ì ì˜ ì •ë³´ë¥¼ íŠ¹í™”í•˜ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤."""

# Streamlit App UI
@st.cache_data 
def load_image(name: str):
    return Image.open(ASSETS / name)

st.set_page_config(page_title="2025ë…„ ë¹…ì½˜í…ŒìŠ¤íŠ¸ AIë°ì´í„° í™œìš©ë¶„ì•¼ - SAVAGE")

def clear_chat_history():
    st.session_state.messages = [SystemMessage(content=system_prompt), AIMessage(content=greeting)]

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.image(load_image("shc_ci_basic_00.png"), width='stretch')
    st.markdown("<p style='text-align: center;'>2025 Big Contest</p>", unsafe_allow_html=True)
    st.markdown("<p style='text-align: center;'>AI DATA í™œìš©ë¶„ì•¼</p>", unsafe_allow_html=True)
    st.write("")
    col1, col2, col3 = st.columns([1,2,1])  # ë¹„ìœ¨ ì¡°ì • ê°€ëŠ¥
    with col2:
        st.button('Clear Chat History', on_click=clear_chat_history)

# í—¤ë” ì»¨í…Œì´ë„ˆ
header_container = st.container()
with header_container:
    st.title("ì‹ í•œì¹´ë“œ ì†Œìƒê³µì¸ ğŸ”‘ ë¹„ë°€ìƒë‹´ì†Œ")
    st.subheader("#ìš°ë¦¬ë™ë„¤ #ìˆ¨ì€ë§›ì§‘ #ì†Œìƒê³µì¸ #ë§ˆì¼€íŒ… #ì „ëµ .. ğŸ¤¤")
    st.write("")

# ë©”ì‹œì§€ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = [
        SystemMessage(content=system_prompt),
        AIMessage(content=greeting)
    ]

chat_container = st.container()

def render_messages():
    """ëª¨ë“  ë©”ì‹œì§€ë¥¼ ë Œë”ë§í•˜ëŠ” í•¨ìˆ˜"""
    with chat_container:
        for i, message in enumerate(st.session_state.messages):
            if isinstance(message, SystemMessage):
                continue
            elif isinstance(message, HumanMessage):
                with st.chat_message("user"):
                    st.write(message.content)
            elif isinstance(message, AIMessage):
                with st.chat_message("assistant"):
                    try:
                        # JSON íŒŒì‹± ì‹œë„
                        response_data = json.loads(message.content)
                        
                        if isinstance(response_data, dict):
                            response_data = [response_data]
                        
                        # ê° ì„¹ì…˜ì„ ìˆœíšŒí•˜ë©° UIì— ë Œë”ë§
                        for item in response_data:
                            section = item.get("section", "ê²°ê³¼")
                            content = item.get("content", "")
                            basis = item.get("basis", "")

                            st.subheader(f"âœ… {section}")
                            st.markdown(content)
                            
                            if basis:
                                with st.expander("ğŸ’¡ ë°ì´í„° ê¸°ë°˜ ê·¼ê±° ë³´ê¸°"):
                                    st.info(basis)
                            st.divider()

                    except json.JSONDecodeError:
                        st.write(message.content)

# ì´ˆê¸° ë©”ì‹œì§€ ë Œë”ë§
render_messages()

def render_chat_message(role: str, content: str):
    with st.chat_message(role):
        st.markdown(content.replace("<br>", "  \n"))

# LLM ëª¨ë¸ ì„ íƒ
llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",  # ìµœì‹  Gemini 2.5 Flash ëª¨ë¸
        google_api_key=GOOGLE_API_KEY,
        temperature=0.1
    )

# MCP ì„œë²„ íŒŒë¼ë¯¸í„°(í™˜ê²½ì— ë§ê²Œ ëª…ë ¹ ìˆ˜ì •)
server_params = StdioServerParameters(
    command="uv",
    args=["run","mcp_server.py"],
    env=None
)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
async def process_user_input():
    """ì‚¬ìš©ì ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ” async í•¨ìˆ˜"""
    async with stdio_client(server_params) as (read, write):
        # ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ClientSessionì„ ë§Œë“¤ê³ 
        async with ClientSession(read, write) as session:
            # ì„¸ì…˜ì„ initialize í•œë‹¤
            await session.initialize()

            # MCP íˆ´ ë¡œë“œ
            tools = await load_mcp_tools(session)

            # ì—ì´ì „íŠ¸ ìƒì„±
            agent = create_react_agent(llm, tools)

            # ì—ì´ì „íŠ¸ì— ì „ì²´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì „ë‹¬
            agent_response = await agent.ainvoke({"messages": st.session_state.messages})
            
            # AI ì‘ë‹µì„ ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            ai_message = agent_response["messages"][-1]  # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ AI ì‘ë‹µ

            return ai_message.content
            

input_container = st.container()
with input_container:
    if query := st.chat_input("ê°€ë§¹ì  ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append(HumanMessage(content=query))
        st.rerun() # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ í‘œì‹œ

# ì‚¬ìš©ì ì…ë ¥ì´ ìˆì„ ë•ŒëŠ” ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ì¶”ê°€í•˜ì—¬ UI ìœ ì§€
if len(st.session_state.messages) > 2:  
    last_message = st.session_state.messages[-1]
    if isinstance(last_message, HumanMessage):
        with st.spinner("Thinking..."):
            try:
                reply = asyncio.run(process_user_input())
                st.session_state.messages.append(AIMessage(content=reply))
                # ë©”ì‹œì§€ ì¶”ê°€ í›„ ë‹¤ì‹œ ë Œë”ë§
                render_messages()
                st.rerun()  # ìƒˆë¡œìš´ ì‘ë‹µì„ í‘œì‹œí•˜ê¸° ìœ„í•´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                
            except* Exception as eg:
                for i, exc in enumerate(eg.exceptions, 1):
                    error_msg = f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ #{i}: {exc!r}"
                    st.session_state.messages.append(AIMessage(content=error_msg))
                render_messages()
                st.rerun()

